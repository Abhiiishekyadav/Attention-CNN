{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynq import Overlay\n",
    "from pynq import allocate\n",
    "import numpy as np\n",
    "overlay = Overlay('attention.bit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynq import ps\n",
    "\n",
    "print(ps.Clocks.fclk0_mhz)\n",
    "ps.Clocks.fclk0_mhz = 375\n",
    "print(ps.Clocks.fclk0_mhz)\n",
    "print(ps.Clocks.cpu_mhz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = overlay.CNN_0\n",
    "mmio = ip.mmio\n",
    "register_map = ip.register_map\n",
    "registers = register_map._register_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, reg in registers.items():\n",
    "    print(name, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocated buffer (m_axi)\n",
    "input_buffer_size = 7840000\n",
    "output_buffer_size = 10000\n",
    "\n",
    "input_buffer = allocate(shape=(input_buffer_size,), dtype=np.int8) \n",
    "output_buffer = allocate(shape=(output_buffer_size,), dtype=np.int8) \n",
    "register_map.im_1.im = input_buffer.device_address\n",
    "register_map.out_r_1.out_r = output_buffer.device_address\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "bias = []\n",
    "weight = []\n",
    "scales = [512, 256, 256,128,128]\n",
    "layers = [150528,802816,401408,200704,100352,25088]\n",
    "\n",
    "x_test = (np.load('x_test.npy')//32).astype(np.int8)\n",
    "y_test = np.load('y_test.npy')\n",
    "\n",
    "for filename in sorted(glob.glob(\"../VitisAI/dump_results/dump_results_weights/quant_conv_*_bias.txt\")):\n",
    "    bias.append(np.loadtxt(filename))\n",
    "for filename in sorted(glob.glob(\"../VitisAI/dump_results/dump_results_weights/quant_conv_*_kernel.txt\")):\n",
    "    weight.append(np.loadtxt(filename))\n",
    "\n",
    "for filename in sorted(glob.glob(\"../VitisAI/dump_results/dump_results_weights/quant_attention_*_bias.txt\")):\n",
    "    bias.append(np.loadtxt(filename))\n",
    "for filename in sorted(glob.glob(\"../VitisAI/dump_results/dump_results_weights/quant_attention_*_kernel.txt\")):\n",
    "    weight.append(np.loadtxt(filename))\n",
    "\n",
    "for filename in sorted(glob.glob(\"../VitisAI/dump_results/dump_results_weights/quant_dense_*_bias.txt\")):\n",
    "    bias.append(np.loadtxt(filename))\n",
    "for filename in sorted(glob.glob(\"../VitisAI/dump_results/dump_results_weights/quant_dense_*_kernel.txt\")):\n",
    "    weight.append(np.loadtxt(filename))\n",
    "\n",
    "for i in range(3):\n",
    "   weight[i] = weight[i].reshape(layers[i],layers[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardware accelerated function\n",
    "def attention_hw(im):\n",
    "    # Write to input buffer\n",
    "    input_buffer[:len(im)] = im\n",
    "    # Send start signal\n",
    "    register_map.CTRL.AP_START = 1\n",
    "    \n",
    "    # Wait until algorithm has completed\n",
    "    while (register_map.CTRL.AP_DONE == 0):\n",
    "        pass\n",
    "\n",
    "    return output_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_sw(im):\n",
    "    result = []\n",
    "    for i in range(10000):\n",
    "        data = im[i]\n",
    "        for j in range(3):\n",
    "            data = (data@weight[j]+bias[j])//scales[j]\n",
    "            if j != 2:\n",
    "                data = data*(data>0)\n",
    "        result.append(np.argmax(data))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "res_hls = []\n",
    "res_py = []\n",
    "err_hls = 0\n",
    "err_py = 0\n",
    "\n",
    "res_hls = attention_hw(x_test.flatten())\n",
    "res_py = attention_sw(x_test)\n",
    "    \n",
    "for i in range(10000):            \n",
    "    if res_hls[i] != y_test[i]:\n",
    "        err_hls +=1\n",
    "\n",
    "    if res_py[i] != y_test[i]:\n",
    "        err_py +=1\n",
    "        \n",
    "print(\"acc hls {}\".format(1-err_hls/10000))                 \n",
    "print(\"acc py {}\".format(1-err_py/10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_time = %timeit -n 1 -r 10 -o mnist_hw(x_test.flatten())\n",
    "sw_time = %timeit -n 1 -r 10 -o mnist_sw(x_test)\n",
    "\n",
    "print('Performance gain:', sw_time.average / hw_time.average) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hw fps = {:.1f}\".format((hw_time.average/10000)**-1))\n",
    "print(\"sw fps = {:.1f}\".format((sw_time.average/10000)**-1))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
